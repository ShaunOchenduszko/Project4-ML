{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv ('Final_Data_w_Colors.csv')\n",
    "pd.set_option('display.max_columns', None)\n",
    "df = df.drop(columns=['Unnamed: 0', 'Unnamed: 0.1', 'background_image','name', 'total_reviews','owners'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.get_dummies(df, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning the review_score, it will be the target of our machine learning.\n",
    "# The idea is we are seeing what features of our games dataset: Genre, Boxart, Release Month, etc. could help predict how well recieved the game is.\n",
    "bins = [-0.1,.60,.70,.80,.90,1]\n",
    "group_names = ['1','2','3','4','5']\n",
    "df2['target'] = pd.cut(df2['review_score'], bins, labels = group_names)\n",
    "df2 = df2.drop(columns=['review_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting out the target to get X and Y data\n",
    "X = df2.drop('target', axis=1)\n",
    "y = df2['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding our target variable for machine learning.\n",
    "num_classes = 6\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Because the data is fairly sparse, a lasso regression would be appropriate model.\n",
    "# lasso_reg = Lasso(max_iter=2000).fit(X_train_scaled, y_train)\n",
    "# lasso_reg.score(X_test_scaled, y_test)\n",
    "\n",
    "# lasso_sel = SelectFromModel(lasso_reg)\n",
    "# lasso_sel.fit(X_train_scaled, y_train)\n",
    "\n",
    "# SelectFromModel(estimator=Lasso(alpha=1.0, copy_X=True, fit_intercept=True,\n",
    "#                                 max_iter=1000, normalize=False, positive=False,\n",
    "#                                 precompute=False, random_state=None,\n",
    "#                                 selection='cyclic', tol=0.0001,\n",
    "#                                 warm_start=False), max_features=None, norm_order=1, prefit=False, threshold=None)\n",
    "\n",
    "# X_lasso_train, X_lasso_test, y_train, y_test = train_test_split(lasso_sel.transform(X), y, random_state=1)\n",
    "# scaler = StandardScaler().fit(X_lasso_train)\n",
    "# X_lasso_train_scaled = scaler.transform(X_lasso_train)\n",
    "# X_lasso_test_scaled = scaler.transform(X_lasso_test)\n",
    "\n",
    "# log_reg = LogisticRegression()\n",
    "# log_reg.fit(X_lasso_train_scaled, y_train)\n",
    "# print(f'Training Score: {log_reg.score(X_lasso_train_scaled, y_train)}')\n",
    "# print(f'Testing Score: {log_reg.score(X_lasso_test_scaled, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': [True, False],\n",
      " 'max_depth': [0, 25, 50, 75, 100, None],\n",
      " 'max_features': ['auto', 'sqrt'],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'min_samples_split': [2, 5, 10],\n",
      " 'n_estimators': [100, 200, 300, 400, 500]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 500, num = 5)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(0, 100, num = 5)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 50 candidates, totalling 100 fits\n"
     ]
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "clf = RandomForestClassifier()\n",
    "# Random search of parameters, using 2 fold cross validation, \n",
    "# search across 50 different combinations, and use all available cores\n",
    "clf_random = RandomizedSearchCV(estimator = clf, param_distributions = random_grid, n_iter = 50, cv = 2, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "clf_random.fit(X_train_scaled, y_train)\n",
    "\n",
    "# n_iter is the number of interations we try\n",
    "#cv is the number of cross validation folds we will try, to reduce chances of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_best = RandomForestClassifier(n_estimators = 200, min_samples_split = 2, min_samples_leaf = 1, max_depth=None, bootstrap=False).fit(X_train_scaled, y_train)\n",
    "print(f'Training Score: {clf_best.score(X_train_scaled, y_train)}')\n",
    "print(f'Testing Score: {clf_best.score(X_test_scaled, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = clf_best.feature_importances_\n",
    "\n",
    "features = sorted(zip(X.columns, clf_best.feature_importances_), key = lambda x: x[1])\n",
    "cols = [f[0] for f in features]\n",
    "width = [f[1] for f in features]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10,20)\n",
    "plt.margins(y=0.001)\n",
    "\n",
    "ax.barh(y=cols, width=width)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = SelectFromModel(clf_best)\n",
    "sel.fit(X_train_scaled, y_train)\n",
    "\n",
    "X_selected_train, X_selected_test, y_train, y_test = train_test_split(sel.transform(X), y, random_state=1)\n",
    "scaler = StandardScaler().fit(X_selected_train)\n",
    "X_selected_train_scaled = scaler.transform(X_selected_train)\n",
    "X_selected_test_scaled = scaler.transform(X_selected_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression()\n",
    "logistic.fit(X_selected_train_scaled, y_train)\n",
    "print(f'Training Score: {logistic.score(X_selected_train_scaled, y_train)}')\n",
    "print(f'Testing Score: {logistic.score(X_selected_test_scaled, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns = df2.columns.tolist()\n",
    "df_columns.remove('target')\n",
    "low_value_columns = []\n",
    "for column in df_columns: \n",
    "    if df2[column].sum() <= 500:\n",
    "        low_value_columns.append(column)\n",
    "print(low_value_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_value = df2.drop(columns=low_value_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting out the target to get X and Y data\n",
    "X = df_value.drop('target', axis=1)\n",
    "y = df_value['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# One hot encoding our target variable for machine learning.\n",
    "num_classes = 6\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_best = RandomForestClassifier(n_estimators = 200, min_samples_split = 2, min_samples_leaf = 1, max_depth=None, bootstrap=False).fit(X_train_scaled, y_train)\n",
    "print(f'Training Score: {clf_best.score(X_train_scaled, y_train)}')\n",
    "print(f'Testing Score: {clf_best.score(X_test_scaled, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "nn_model = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_model.add(tf.keras.layers.Dense(units=230, activation=\"relu\", input_dim=92)) \n",
    "\n",
    "# Second hidden layer\n",
    "nn_model.add(tf.keras.layers.Dense(units=62, activation=\"relu\")) \n",
    "\n",
    "# Output layer\n",
    "nn_model.add(tf.keras.layers.Dense(units=6, activation=\"softmax\")) \n",
    "\n",
    "# Check the structure of the model\n",
    "nn_model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "fit_model = nn_model.fit(X_train_scaled, y_train, epochs=50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn_model.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
